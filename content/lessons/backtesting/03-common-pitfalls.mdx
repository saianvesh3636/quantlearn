---
title: Common Pitfalls
module: backtesting
order: 3
colabUrl: ""
prerequisites:
  - backtesting/02-backtesting-fundamentals
---

# Common Backtesting Pitfalls

Most backtests are wrong. Understanding common mistakes will save you from building strategies that only work in simulation.

<WhyItMatters tldr="A strategy with a Sharpe ratio of 1.59 in backtesting delivered -0.18 in live trading. The difference between these outcomes is understanding the pitfalls in this lesson.">
Here's a sobering fact from academic research: **the more optimization you do, the worse your live performance will be**. A Quantopian study of 888 strategies found that backtested Sharpe ratios had **nearly zero predictive power** for live returns.

The strategies in this lesson aren't theoretical - they're the exact mistakes that have cost quantitative funds billions of dollars. Learn them, or repeat them.
</WhyItMatters>

<ConceptCard title="The Cardinal Sin" type="warning">
**Lookahead Bias** is using information that wouldn't have been available at the time of the trade. This makes backtests look amazing but strategies fail live.
</ConceptCard>

## Lookahead Bias

<ConceptCard title="Examples of Lookahead" type="example">
- Using end-of-day prices to make decisions that had to be made intraday
- Including companies in your universe that you only know survived because you're looking backward
- Using adjusted prices without accounting for when adjustments were announced
</ConceptCard>

<InteractiveExample title="Lookahead vs Reality" description="Strategy performance with and without lookahead bias">
  <EquityCurve
    data={[
      { date: "2020-01", equity: 100000, drawdown: 0 },
      { date: "2020-04", equity: 125000, drawdown: 0 },
      { date: "2020-07", equity: 150000, drawdown: 0 },
      { date: "2020-10", equity: 180000, drawdown: 0 },
      { date: "2021-01", equity: 220000, drawdown: 0 },
      { date: "2021-04", equity: 270000, drawdown: 0 }
    ]}
    title="With Lookahead Bias (Fake Results)"
  />
</InteractiveExample>

<InteractiveExample title="Same Strategy Without Lookahead" description="Reality is much less impressive">
  <EquityCurve
    data={[
      { date: "2020-01", equity: 100000, drawdown: 0 },
      { date: "2020-04", equity: 98000, drawdown: -2 },
      { date: "2020-07", equity: 105000, drawdown: 0 },
      { date: "2020-10", equity: 102000, drawdown: -2.9 },
      { date: "2021-01", equity: 108000, drawdown: 0 },
      { date: "2021-04", equity: 103000, drawdown: -4.6 }
    ]}
    title="Without Lookahead Bias (Real Results)"
  />
</InteractiveExample>

## Survivorship Bias

<ConceptCard title="Survivorship Bias" type="definition">
Testing only on stocks that still exist today ignores all the companies that went bankrupt. This systematically overstates returns because you're only seeing the winners.
</ConceptCard>

## Overfitting

<FormulaDisplay
  formula="In\text{-}Sample\;R^2 >> Out\text{-}Sample\;R^2"
  explanation="A classic sign of overfitting: great fit on training data, poor fit on new data."
  variables={[
    { symbol: "R^2", meaning: "Coefficient of determination (goodness of fit)" }
  ]}
/>

<ConceptCard title="Signs of Overfitting" type="warning">
- Strategy has many parameters (more than 3-4 is suspicious)
- Performance is amazing in-sample but poor out-of-sample
- Strategy only works on specific date ranges
- Tiny changes in parameters cause huge performance swings
</ConceptCard>

<InteractiveExample title="Overfitting Visualization" description="Too many parameters fit noise, not signal">
  <SensitivityHeatmap
    data={{
      param1Values: [5, 10, 15],
      param1Label: "Parameter A",
      param2Values: [10, 20, 30],
      param2Label: "Parameter B",
      results: [
        [0.8, 1.2, 0.5],
        [1.5, 2.8, 0.9],
        [0.7, 1.1, 0.4]
      ],
      metricLabel: "Sharpe Ratio"
    }}
    title="Sharpe Ratio by Parameter Values"
    highlightOptimal={true}
    showOverfitZone={true}
    overfitThreshold={2.0}
  />
</InteractiveExample>

<RealWorldExample
  title="Academic Proof: Backtested Returns Are Meaningless"
  date="2015-2020 Research"
  impact="Sharpe 1.59 → -0.18 out-of-sample"
  sources={[
    { name: "Bailey & López de Prado", url: "https://www.davidhbailey.com/dhbpapers/overfit-tools-at.pdf" },
    { name: "Lawrence Berkeley National Lab" }
  ]}
  lesson="With enough optimization, you can make any random data look profitable. The more parameters you tweak, the more you're fitting noise."
>
Researchers Bailey and López de Prado demonstrated this devastatingly:

- A strategy optimized to show **Sharpe Ratio of 1.59** in-sample
- Delivered **-0.18** on out-of-sample data (a losing strategy)
- The more optimization performed, the worse live performance became

**Why this happens:** Modern computers can test millions of strategy variations. Some will inevitably perform well by chance on historical data. These strategies are fitting noise, not signal - and noise doesn't persist.

**Von Neumann's warning:** "With four parameters I can fit an elephant, and with five I can make him wiggle his trunk."
</RealWorldExample>

<RealWorldExample
  title="Knight Capital: $440 Million Lost in 45 Minutes"
  date="August 1, 2012"
  impact="$440M loss, company acquired"
  sources={[
    { name: "SEC Press Release", url: "https://www.sec.gov/newsroom/press-releases/2013-222" },
    { name: "SEC Administrative Order" }
  ]}
  lesson="Even after surviving backtesting pitfalls, implementation bugs can destroy you. This is why paper trading and incremental rollout matter."
>
Knight Capital's disaster wasn't about strategy - it was about deployment:

- A technician failed to deploy new code to 1 of 8 servers
- Old "Power Peg" code from 2005 was accidentally triggered
- 212 customer orders generated **4 million erroneous market orders**
- Knight accumulated $3.5B long and $3.15B short positions in 45 minutes
- **$440 million loss** before they could stop it

**The fatal flaw:** Legacy code was never removed. No kill switch existed to halt runaway trading. The strategy might have been fine - but the implementation killed the company.

**SEC outcome:** First enforcement action under Rule 15c3-5. Knight paid $12M settlement and was acquired by Getco four months later.
</RealWorldExample>

<KeyTakeaway>
The three deadly sins of backtesting: **Lookahead bias** (using future information), **Survivorship bias** (ignoring failures), and **Overfitting** (fitting noise). Always ask: "Would I have known this at the time?"
</KeyTakeaway>

<FindYourEdge
  concept="Robustness Testing"
  strategies={[
    {
      name: "Parameter Sensitivity Analysis",
      description: "A robust strategy works across a range of parameters. If only one exact parameter combination works, you've fit noise.",
      implementation: "Vary each parameter ±20%. Plot performance surface. Look for smooth 'plateaus' of good performance, not sharp 'peaks'. Only trade parameters in the middle of stable regions.",
      parameters: "Test: lookback ± 20%, thresholds ± 20%, all combinations. Require: >50% of nearby parameters also profitable.",
      evidence: "AQR, Two Sigma, and academic research all emphasize parameter stability as key robustness indicator. Sharp peaks indicate overfitting.",
      risks: "You'll reject many 'good' backtests. May end up with more conservative strategies. But stable strategies are more likely to work out-of-sample."
    },
    {
      name: "Sub-Period Consistency",
      description: "Split your data into 3-5 sub-periods. A robust strategy should work in each period, not just overall.",
      implementation: "Divide data into sub-periods: pre-2000, 2000-2007, 2008-2009, 2010-2019, 2020+. Test strategy in each. Require positive Sharpe in majority of periods.",
      parameters: "Minimum: 3 sub-periods. Require: Positive returns in at least 2/3 of periods. Flag: Any period with losses > 20%.",
      evidence: "Strategies that only work in one period are likely fit to that specific regime. Multi-period consistency is a strong indicator of genuine edge.",
      risks: "Reduces statistical power by splitting data. May reject strategies that are 'right for current regime'. Requires judgment about what periods to use."
    }
  ]}
/>

<ThinkAboutIt questions={[
  {
    question: "If you test 100 random strategies, about how many would you expect to show a 'statistically significant' result by pure chance?",
    hint: "At a 5% significance level, about 5 strategies will look significant purely by chance. This is why multiple testing correction (like Bonferroni) is essential - but rarely used in practice."
  },
  {
    question: "You find a strategy with Sharpe 2.0 in backtesting. What questions should you ask before trading it live?",
    hint: "How many parameters were optimized? How many strategies were tested to find this one? What's the out-of-sample performance? Does it work across different time periods and markets? Is the signal economically sensible?"
  },
  {
    question: "Why do you think survivorship bias causes bigger problems for small-cap stock strategies than large-cap?",
    hint: "Small companies fail more often. If you're backtesting on today's small-cap universe, you're missing all the bankruptcies. A 'buy small distressed stocks' strategy would look amazing in backtest but terrible in reality."
  },
  {
    question: "Knight Capital lost $440M due to a deployment bug. How would you design a system to prevent this?",
    hint: "Consider: kill switches, position limits, gradual rollout, code review, removal of legacy code, monitoring alerts, human oversight for the first N minutes of new deployments."
  }
]} />
